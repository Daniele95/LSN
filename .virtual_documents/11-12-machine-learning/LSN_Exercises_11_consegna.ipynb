








import numpy as np
import matplotlib.pyplot as plt


# generate training inputs
np.random.seed(0)
x_train = np.random.uniform(-1, 1, 500)
x_valid = np.random.uniform(-1, 1, 50)
x_valid.sort()

m = 2 # slope
b = 1 # intersect
y_target = m * x_valid + b # ideal (target) linear function

sigma = [0.1, 0.3, 0.5] # noise standard deviation, for the moment it is absent
sigmas= ["0.1", "0.3", "0.5"]

y_train = np.zeros((3, len(x_train)))  # Define y_train as a matrix
y_valid = np.zeros((3, len(x_valid)))  # Define y_valid as a matrix

plt.figure(figsize=(17,5))
for i in range (3):
    plt.subplot(1,3,i+1)
    y_train[i] = np.random.normal(m * x_train + b, sigma[i]) # actual measures from which we want to guess regression parameters
    y_valid[i] = np.random.normal(m * x_valid + b, sigma[i])
    plt.scatter(x_train, y_train[i], marker = "x", color='green', label='Training data',linewidth=0.5, alpha=0.4)
    plt.plot(x_valid, y_target, label='target')
    plt.scatter(x_valid, y_valid[i], color='r', label='validation data', marker=".")
    plt.title("$\sigma=$"+sigmas[i], fontsize=20)
    plt.legend()
    plt.grid(True)
      
plt.show()





import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 10))

Nepochs = 40
models = []  # List to store the models

for i in range(3):
    model = Sequential()
    model.add(Dense(1, input_shape=(1,)))

    # compile the model choosing optimizer, loss, and metrics objects
    model.compile(optimizer='sgd', loss='mse', metrics=['mse'])

    # create history
    history = model.fit(
        x=x_train,
        y=y_train[i],
        batch_size=32,
        epochs=Nepochs,
        shuffle=True,
        validation_data=(x_valid, y_valid[i]),
        verbose=0
    )
    
    # evaluate model
    score = model.evaluate(x_valid, y_valid[i], batch_size=32, verbose=1)
    print('Test loss:', score[0])
    print('Test accuracy:', score[1])
    
    models.append(model)  # Save the model in the list

    plt.subplot(3, 1, i + 1)
    plt.plot(history.history['loss'], color="darkblue")
    plt.plot(history.history['val_loss'], color="green")
    plt.axhline(0, color='red', alpha=0.6)
    plt.title("$\sigma=$" + sigmas[i], fontsize=20)
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='best')

    plt.grid(True)
    
plt.subplots_adjust(hspace=0.5)  
plt.show()






for i in range(3):
    plt.figure(figsize=(15, 10))    
    plt.subplot(3, 1, i + 1)
    x_predicted = np.random.uniform(-2, 1, 100)
    y_predicted = models[i].predict(x_predicted)
    plt.scatter(x_predicted, y_predicted, color='r', label='Predicted')
    plt.plot(x_valid, y_target, label='Target')
    plt.title("$\sigma=$" + sigmas[i], fontsize=20)
    plt.legend()
    plt.grid(True)
    plt.show()








import numpy as np
import matplotlib.pyplot as plt

def f(x):
    return a + b*x + c*x**2 + d*x**3

# parameters of f(x) = a+bx+cx^2+dx^3
a = 4 
b = -2 
c = -3
d = 3

# generate training inputs
Ntrain=3000
Nepochs=30
sigma=0.2

np.random.seed(0)
x_train = np.random.uniform(-1, 1, Ntrain)
x_valid = np.random.uniform(-1, 1, 300)
x_valid.sort()
y_target = f(x_valid) # ideal (target) polynomial

y_train = np.zeros((3, len(x_train)))  # Define y_train as a matrix
y_valid = np.zeros((3, len(x_valid)))  # Define y_valid as a matrix

for i in range (3):
    y_train[i] = np.random.normal(f(x_train), sigma) # actual measures from which we want to guess regression parameters
    y_valid[i] = np.random.normal(f(x_valid), sigma)


plt.figure(figsize=(14,5))
plt.scatter(x_train, y_train[0], marker = "x", color='green', label='Training data',linewidth=0.5, alpha=0.4)
plt.plot(x_valid, y_target, label = "Analytic function\n $f(x) = 4 - 3x - 2x^2 + 3x^3$", color="darkblue")
plt.scatter(x_valid, y_valid[0], marker = ".", color='r', label='Validation data')
plt.xlabel("x")
plt.ylabel("y")
plt.title("Analytic function, validation data and training data", fontweight = "bold")
plt.grid()
plt.legend()
plt.show()


import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 10))

Nepochs = 40
models = []  # List to store the models
layers = ["Two", "Three", "Four"]

for i in range(3):
    model = Sequential()
    
    act = 'relu'
    model.add(Dense(20, input_shape=(1,), activation = act)) # nota: funzione di attivazione (esplorare quali vanno meglio)
    if (i>0):
        model.add(Dense(30, activation = act
    - cudatoolkit=11.2
    - cudnn=8.1.0


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    cudatoolkit-11.2.2         |      hc23eb0c_13       630.6 MB  conda-forge
    cudnn-8.1.0.77             |       h90431f1_0       634.8 MB  conda-forge
    ------------------------------------------------------------
                                           Total:        1.24 GB

The following NEW packages will be INSTALLED:

  cudatoolkit        conda-forge/linux-64::cudatoolkit-11.2.2-hc23eb0c_13 
  cudnn              conda-forge/linux-64::cudnn-8.1.0.77-h90431f1_0 


Proceed ([y]/n)? y


Downloading and Extracting Packages:
))
    if (i>1):
        model.add(Dense(30, activation = act))
    model.add(Dense(1, activation = 'relu'))

    # compile the model choosing optimizer, loss, and metrics objects
    model.compile(optimizer='sgd', loss='mse', metrics=['mse'])

    # create history
    history = model.fit(
        x=x_train,
        y=y_train[i],
        batch_size=32,
        epochs=Nepochs,
        shuffle=True,
        validation_data=(x_valid, y_valid[i]),
        verbose=0
    )
    
    # evaluate model
    score = model.evaluate(x_valid, y_valid[i], batch_size=32, verbose=1)
    print('Test loss:', score[0])
    
    models.append(model)  # Save the model in the list

    plt.subplot(3, 1, i + 1)
    plt.plot(history.history['loss'], color="darkblue")
    plt.plot(history.history['val_loss'], color="green")
    plt.axhline(0, color='red', alpha=0.6)
    plt.title(layers[i]+" layers", fontsize=20)
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='best')

    plt.grid(True)
    
plt.subplots_adjust(hspace=0.5)  
plt.show()






plt.figure(figsize=(14,5))

x_predicted = np.random.uniform(-1.5, 1.5, 100)
y_predicted = models[1].predict(x_predicted)
plt.scatter(x_predicted, y_predicted,color='r', label='Predicted data')
plt.scatter(x_valid, y_valid[1], marker = "x", color='green', label='Validation data', linewidth=0.5)
plt.plot(x_valid, f(x_valid), label='Analytic function', color="darkblue")
plt.legend()
plt.grid(True)
plt.show()








import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d

def f(x,y):
    return np.sin(x**2+y**2)


# generate training inputs
Ntrain=10000
Nepochs=30
Nvalidation = int(Ntrain/10)
#Nvalidation = 1000
sigma=0.2

np.random.seed(0)
span=3/2

x_train = np.random.uniform(-span, span, Ntrain)
y_train = np.random.uniform(-span, span, Ntrain)

x_valid = np.random.uniform(-span, span, Nvalidation)
y_valid = np.random.uniform(-span, span, Nvalidation)

XY_train = np.column_stack((x_train,y_train))      # in the format needed
XY_valid = np.column_stack((x_valid,y_valid))      #   by `model.fit` method

z_train = np.random.normal(f(x_train,y_train), sigma) 
z_valid = np.random.normal(f(x_valid,y_valid), sigma)


fig = plt.figure(figsize=(15,12))
ax = fig.add_subplot(111, projection='3d')

x_ = np.arange(-span,span, 0.1)
y_ = np.arange(-span,span, 0.1)
X_, Y_ = np.meshgrid(x_,y_)
ax.plot_wireframe(  X_, Y_, f(X_,Y_), 
                    rstride=2, label='Analytic function\n $\sin(x^2+y^2)$', color = 'darkblue',
                    alpha =0.4)

ax.scatter(x_valid, y_valid, z_valid, marker = ".", color = "r", label='Validation data')
ax.scatter(x_train, y_train, z_train, marker = "x", color = "green",
               label='Training data', linewidth=0.3, alpha = 0.5)


ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
ax.set_title('Surface Plot')
ax.view_init(elev=15, azim=25)  # Adjust the elevation (vertical angle) and azimuth (horizontal angle)

plt.legend()
plt.show()





import tensorflow as tf
from tensorflow import keras

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras import backend as K
from tensorflow.keras.utils import get_custom_objects

model = tf.keras.Sequential()

act = 'tanh'
model.add(Dense(40, input_shape=(2,), activation = act)) # nota: funzione di attivazione (esplorare quali vanno meglio)
model.add(Dense(30, activation = act))
model.add(Dense(20, activation = act))
model.add(Dense(1, activation = act))


# compile the model choosing optimizer, loss and metrics objects
model.compile(optimizer='nadam', loss='mse', metrics=['mse'])


history = model.fit(x=XY_train, y=z_train, 
          batch_size=32, epochs=Nepochs,
          shuffle=True, # a good idea is to shuffle input before at each epoch
          validation_data=(XY_valid, z_valid),verbose=0)


# evaluate model
score = model.evaluate(XY_valid, z_valid, batch_size=32, verbose=0)
#verbose=1: It controls the verbosity mode. In this case, verbose=1 means that the evaluation 
#progress and results will be displayed during the evaluation.

# print performance
print()
print('Test loss:', score[0])
print('Test accuracy:', score[1])


# look into training history
# Plot training & validation loss values
plt.figure(figsize=(14,5))

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='best')
plt.show()





# generate predictions
x_predicted = np.random.uniform(-span, span, 2000) 
y_predicted = np.random.uniform(-span, span, 2000) 
XY_predicted = np.column_stack((x_predicted,y_predicted))
z_predicted = model.predict(XY_predicted)

# plot
x_ = np.arange(-span,span, 0.1)
y_ = np.arange(-span,span, 0.1)
X_, Y_ = np.meshgrid(x_,y_)

fig = plt.figure(figsize = (15,10))
ax = fig.add_subplot(111, projection='3d')
fig.add_axes(ax)
ax.plot_wireframe(  X_, Y_, f(X_,Y_), 
                    rstride=2, cstride=2, 
                    label='Target function\n $\sin(x^2+y^2)$',
                    alpha = 0.2)
ax.scatter(x_predicted, y_predicted, z_predicted, marker = ".", color = "r", label='Validation data (for test)')
ax.set_xlabel("x", fontsize = 18)
ax.set_ylabel("y", fontsize = 18)
ax.set_zlabel("z", fontsize = 18)
plt.title("Validation data and Target f
    - cudatoolkit=11.2
    - cudnn=8.1.0


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    cudatoolkit-11.2.2         |      hc23eb0c_13       630.6 MB  conda-forge
    cudnn-8.1.0.77             |       h90431f1_0       634.8 MB  conda-forge
    ------------------------------------------------------------
                                           Total:        1.24 GB

The following NEW packages will be INSTALLED:

  cudatoolkit        conda-forge/linux-64::cudatoolkit-11.2.2-hc23eb0c_13 
  cudnn              conda-forge/linux-64::cudnn-8.1.0.77-h90431f1_0 


Proceed ([y]/n)? y


Downloading and Extracting Packages:
unction (from before)", fontsize = 18, fontweight = "bold")
ax.legend()
ax.view_init(10, 30)


plt.show()










